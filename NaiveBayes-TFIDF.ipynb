{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import Sastrawi\n",
    "# import nltk\n",
    "\n",
    "import re\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from nltk.tag import CRFTagger\n",
    "# from collections import Counter\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lexicon_score_to_word(score):\n",
    "    if score > 0:\n",
    "        return \" \".join(['selamat'] * score)\n",
    "    else:\n",
    "        return \" \".join(['bodoh'] * -score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticon_score_to_word(score):\n",
    "    if score > 0:\n",
    "        return \" \".join(['selamat'] * score)\n",
    "    else:\n",
    "        return \" \".join(['bodoh'] * -score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('tdp.csv', encoding='Latin')\n",
    "tambahan_data = pd.read_csv('tambahan_preprocess.csv', encoding='Latin')[0:120]\n",
    "train_data = pd.concat([train_data, tambahan_data], ignore_index=True)\n",
    "train_data['tweet'].fillna('', inplace=True)\n",
    "tester_data = pd.read_csv('tdp_test.csv', encoding='Latin')\n",
    "tester_data['tweet'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "train_data_tfidf = tfidf.fit_transform(train_data['tweet'])\n",
    "tester_data_tfidf = tfidf.transform(tester_data['tweet'])\n",
    "\n",
    "train_data['temp'] = train_data['lexicon_score'].apply(convert_lexicon_score_to_word)\n",
    "train_data['temp2'] = train_data['emoticon_score'].apply(convert_emoticon_score_to_word)\n",
    "train_data['tweet'] = train_data['tweet'] + ' ' + train_data['temp'] + ' ' + train_data['temp2']\n",
    "\n",
    "tester_data['temp'] = tester_data['lexicon_score'].apply(convert_lexicon_score_to_word)\n",
    "tester_data['temp2'] = tester_data['lexicon_score'].apply(convert_emoticon_score_to_word)\n",
    "\n",
    "tester_data['tweet'] = tester_data['tweet'] + ' ' + tester_data['temp'] + ' ' + tester_data['temp2']\n",
    "\n",
    "train_data_tfidf_df = pd.DataFrame(train_data_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "tester_data_tfidf_df = pd.DataFrame(tester_data_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tfidf_df['lexicon_score'] = train_data['lexicon_score']\n",
    "train_data_tfidf_df['emoticon_score'] = train_data['emoticon_score']\n",
    "train_data_tfidf_df['jj'] = train_data['jj']\n",
    "train_data_tfidf_df['neg'] = train_data['neg']\n",
    "train_data_tfidf_df['exclamation'] = train_data['exclamation']\n",
    "train_data_tfidf_df['ortografi'] = train_data['ortografi']\n",
    "train_data_tfidf_df['lexicon_pos_score'] = train_data['lexicon_pos_score']\n",
    "train_data_tfidf_df['lexicon_neg_score'] = train_data['lexicon_neg_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_data_tfidf_df['lexicon_score'] = tester_data['lexicon_score']\n",
    "tester_data_tfidf_df['emoticon_score'] = tester_data['emoticon_score']\n",
    "tester_data_tfidf_df['jj'] = tester_data['jj']\n",
    "tester_data_tfidf_df['neg'] = tester_data['neg']\n",
    "tester_data_tfidf_df['exclamation'] = tester_data['exclamation']\n",
    "tester_data_tfidf_df['ortografi'] = tester_data['ortografi']\n",
    "tester_data_tfidf_df['lexicon_pos_score'] = tester_data['lexicon_pos_score']\n",
    "tester_data_tfidf_df['lexicon_neg_score'] = tester_data['lexicon_neg_score']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "t_svd = TruncatedSVD()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "vocab = tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_svd.fit(train_data_tfidf)\n",
    "best_features = [feature_names[i] for i in t_svd.components_[0].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = best_features[0:1740] + ['lexicon_score', 'emoticon_score', 'neg', 'jj']\n",
    "target = 'sentimen'\n",
    "\n",
    "X, y = train_data_tfidf_df[features], train_data[target].values\n",
    "Xx = tester_data_tfidf_df[best_features]\n",
    "Xx.to_csv('Xx.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVC, LinearSVC, SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
    "classifier = BernoulliNB()\n",
    "# classifier = VotingClassifier(estimators=[('bnb', BernoulliNB()), ('mnlb', MultinomialNB())], \n",
    "#                               voting='soft')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "print(accuracy_score(classifier.predict(X_test), y_test))\n",
    "\n",
    "classifier.fit(X, y)\n",
    "print(accuracy_score(classifier.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(800, 2000, 10):\n",
    "#     features = best_features[0:j] + ['emoticon_score', 'lexicon_score', 'jj', 'neg']\n",
    "#     target = 'sentimen'\n",
    "\n",
    "#     X, y = train_data_tfidf_df[features], train_data[target].values\n",
    "#     print('------------------' + str(j) + '--------------')\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
    "#     classifier = BernoulliNB()\n",
    "\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     print(accuracy_score(classifier.predict(X_test), y_test))\n",
    "\n",
    "#     classifier.fit(X, y)\n",
    "#     print(accuracy_score(classifier.predict(X_test), y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_data['predicted'] = predicted\n",
    "tester_data.to_csv('result20.csv', index=False, header=False, columns=['tweetID', 'predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "# from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "# from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# classifiers = [\n",
    "#     ('Decission Tree', DecisionTreeClassifier()),\n",
    "#     ('Logistic Regression', LogisticRegression()),\n",
    "#     ('SVM', LinearSVC()),\n",
    "#     ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "#     ('KNN', KNeighborsClassifier()),\n",
    "#     ('Ensemble', GradientBoostingClassifier())\n",
    "# ]\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# train_scores = 0\n",
    "# test_scores = 0\n",
    "\n",
    "# kfold = KFold(n_splits=10, random_state=46)\n",
    "# dt = MultinomialNB()\n",
    "# for train_index, test_index in kfold.split(X):\n",
    "#     X_train, y_train = X[train_index], y[train_index]\n",
    "#     X_test, y_test = X[test_index], y[test_index]\n",
    "#     dt.fit(X_train, y_train)\n",
    "#     train_scores += accuracy_score(dt.predict(X_train), y_train)\n",
    "#     test_scores += accuracy_score(dt.predict(X_test), y_test)\n",
    "    \n",
    "# print(train_scores / 10)\n",
    "# print(test_scores / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = dt.predict(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester_data['predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester_data.to_csv('results14.csv', index=False, header=False, columns=['tweetID', 'predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "classifier = pickle.load(open('model.sav', 'rb'))\n",
    "Xx = pd.read_csv('Xx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = classifier.predict(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
