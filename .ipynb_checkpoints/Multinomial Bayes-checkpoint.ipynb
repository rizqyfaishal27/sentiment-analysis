{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import Sastrawi\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('dataset/train_set.csv', delimiter=',', encoding='Latin')\n",
    "tester_data = pd.read_csv('dataset/test_set.csv', delimiter=',', encoding='Latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = pd.read_csv('emoticon.txt', delimiter='\\t', names=['emoticon', 'emoticon_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_texts = emoticons['emoticon'].tolist()\n",
    "emoticon_scores = emoticons['emoticon_score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticonset = set()\n",
    "for i in range(len(emoticon_texts)):\n",
    "    emoticonset.add((emoticon_texts[i], emoticon_scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortografi_exception(tweet):\n",
    "    special_list = ['[USERNAME]', '[URL]', '[SENSITIVE-NO]']\n",
    "    for sp in special_list:\n",
    "        tweet = tweet.replace(sp, '')\n",
    "    return tweet\n",
    "\n",
    "def extract_ortografi_word_capital_count(tweet):\n",
    "    words = nltk.word_tokenize(ortografi_exception(tweet))\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if all([c.isupper() for c in word]):\n",
    "            count = count + 1\n",
    "    return count / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exclamation_count(tweet):\n",
    "    tweet = re.sub(r'!{1,}', '!', tweet)\n",
    "    return sum((1 for c in tweet if c == \"!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ortografi'] = train_data['tweet'].apply(extract_ortografi_word_capital_count)\n",
    "tester_data['ortografi'] = tester_data['tweet'].apply(extract_ortografi_word_capital_count)\n",
    "\n",
    "train_data['exclamation'] = train_data['tweet'].apply(extract_exclamation_count)\n",
    "tester_data['exclamation'] = tester_data['tweet'].apply(extract_exclamation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def extract_emoticon(tweet):\n",
    "    score = 0\n",
    "    for emoticon_text, emoticon_score in emoticonset:\n",
    "        occurence = 0\n",
    "        if emoticon_text in tweet:\n",
    "            score += emoticon_score\n",
    "        for i in range(len(tweet) - len(emoticon_text) - 1):\n",
    "            if len(emoticon_text) <= len(tweet) and tweet[i:(i+len(emoticon_text))] == emoticon_text:\n",
    "                occurence += 1\n",
    "        score += (occurence * emoticon_score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_emoticon(\"cie andien yang lagi bep marah marah mulu :p :) :) :) :) :-) :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['emoticon_score'] = train_data['tweet'].apply(extract_emoticon)\n",
    "tester_data['emoticon_score'] = tester_data['tweet'].apply(extract_emoticon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbba_ = pd.read_csv('kbba.txt', delimiter='\\t', names=['from', 'to'])\n",
    "kbba_from = kbba_['from'].tolist()\n",
    "kbba_to = kbba_['to'].tolist()\n",
    "\n",
    "kbba_repo = list()\n",
    "for i in range(len(kbba_from)):\n",
    "    kbba_repo.append((kbba_from[i], kbba_to[i]))\n",
    "    \n",
    "abbr_ = pd.read_csv('singkatan-lib.csv', delimiter=',', names=['from', 'to'])\n",
    "abbr_from = abbr_['from'].tolist()\n",
    "abbr_to = abbr_['to'].tolist()\n",
    "\n",
    "abbr_repo = list()\n",
    "for i in range(len(abbr_from)):\n",
    "    abbr_repo.append((abbr_from[i], abbr_to[i]))\n",
    "\n",
    "noises_ = pd.read_csv('noise.txt', names=['noise'])\n",
    "noises_repo = noises_['noise'].tolist()\n",
    "\n",
    "def normalisasi(tweet):\n",
    "    normal_tw = tweet.lower() #lowercase\n",
    "    normal_tw_words = nltk.word_tokenize(normal_tw)\n",
    "    \n",
    "    normal_tw_words_normalized = [\"\"]\n",
    "    for word in normal_tw_words:\n",
    "        match = False\n",
    "        for kbba_f, kbba_t in kbba_repo:\n",
    "            if word == kbba_f:\n",
    "                normal_tw_words_normalized.append(kbba_t)\n",
    "                match = True\n",
    "                break\n",
    "        if not match:\n",
    "            normal_tw_words_normalized.append(word)\n",
    "    \n",
    "    normal_tw = \" \".join(normal_tw_words_normalized)\n",
    "\n",
    "    normal_tw_words = nltk.word_tokenize(normal_tw)\n",
    "    normal_tw_words_normalized = [\"\"]\n",
    "    for word in normal_tw_words:\n",
    "        match = False\n",
    "        for abbr_f, abbr_t in abbr_repo:\n",
    "            if word == abbr_f:\n",
    "                normal_tw_words_normalized.append(abbr_t)\n",
    "                match = True\n",
    "                break\n",
    "        if not match:\n",
    "            normal_tw_words_normalized.append(word)\n",
    "    normal_tw = \" \".join(normal_tw_words_normalized)\n",
    "\n",
    "#     normal_tw_words_normalized = []\n",
    "#     for word in normal_tw_words:\n",
    "#         match = False\n",
    "#         for noise in noises_repo:\n",
    "#             if word == noise:\n",
    "#                 match = True\n",
    "#                 break\n",
    "#         if not match:\n",
    "#             normal_tw_words_normalized.append(word)\n",
    "#     normal_tw = \" \".join(normal_tw_words_normalized)\n",
    "\n",
    "    normal_tw = re.sub('(\\.){1,}', ' ', normal_tw)\n",
    "    normal_tw = re.sub('\\s+', ' ', normal_tw) # remove extra space\n",
    "    normal_tw = normal_tw.strip() #trim depan belakang\n",
    "    normal_tw = re.sub(r'(wk){2,}|(wka){2,}|(ck){2,}|(ha){2,}|(he){2,}', ' emotxtawa ', normal_tw)\n",
    "    normal_tw = re.sub(r'(hiks)|(kiw){2,}|(hu){2,}', ' emotxtangis ', normal_tw)\n",
    "    normal_tw   =   re.sub(r'[^\\w\\s\\.]',' ',normal_tw)   #buang punctuation\n",
    "    normal_tw = re.sub(r'([A-Za-z])\\1{1,}\\s', r'\\1', normal_tw)\n",
    "    normal_tw = re.sub(r'([A-Za-z])\\1{1,}$', r'\\1', normal_tw)\n",
    "    normal_tw = normal_tw.strip()\n",
    "    return normal_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emoticon_2(tweet):\n",
    "    score = 0\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    for word in words:\n",
    "        if word == 'emotxtawa':\n",
    "            score += 1\n",
    "        if word == 'emotxtangis':\n",
    "            score -= 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet'] = train_data['tweet'].apply(normalisasi)\n",
    "tester_data['tweet'] = tester_data['tweet'].apply(normalisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['emoticon_score'] = train_data['emoticon_score'] + train_data['tweet'].apply(extract_emoticon_2)\n",
    "tester_data['emoticon_score'] = tester_data['emoticon_score'] + tester_data['tweet'].apply(extract_emoticon_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_normalized.csv', index=False, header=False)\n",
    "tester_data.to_csv('tester_data_normalized.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formalized = pd.read_csv('dataset/train_data_formalized.csv', encoding='Latin')\n",
    "tester_data_formalized = pd.read_csv('dataset/tester_data_formalized.csv', encoding='Latin')\n",
    "\n",
    "# train_data_formalized = train_data\n",
    "# tester_data_formalized = tester_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('stopwords.txt', header=None)[0].values\n",
    "def remove_stopwords(tweet, stopwords):\n",
    "    special_list = ['username', 'url', 'sensitive-no']\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    token_afterremoval = []\n",
    "    for k in token:\n",
    "        if k not in stopwords and k not in special_list:\n",
    "            token_afterremoval.append(k)\n",
    "    str_clean = ' '.join(token_afterremoval)\n",
    "    return str_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "def stemming(tweet):\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    stem_kalimat = []\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    for k in token:\n",
    "        stem_kata = stemmer.stem(k)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)\n",
    "    return stem_kalimat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(tweets):\n",
    "    temp_tweets = tweets.copy()\n",
    "    temp_tweets['tweet'] = temp_tweets['tweet'].apply(lambda tweet: remove_stopwords(tweet, stopwords))\n",
    "#     temp_tweets['tweet'] = temp_tweets['tweet'].apply(stemming)\n",
    "    return temp_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preprocess = pre_processing(train_data_formalized)\n",
    "tester_data_preprocess = pre_processing(tester_data_formalized)\n",
    "\n",
    "# train_data_preprocess.to_csv('train_data_preprocessed.csv', index=False)\n",
    "# tester_data_preprocess.to_csv('tester_data_preprocessed.csv', index=False)\n",
    "\n",
    "# train_data_preprocess = pd.read_csv('train_data_preprocessed.csv', delimiter=',', encoding='Latin-1', names=['id', 'sentimen', 'tweet', 'capital_count', 'exclamation_count', 'word_count', 'char_count', 'word_capital_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = set(pd.read_csv('positif_vania.txt', names=['word'])['word'].tolist())\n",
    "negatives = set(pd.read_csv('negatif_vania.txt', names=['word'])['word'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "ct.set_model_file(\"all_indo_man_tag_corpus_model.crf.tagger\")\n",
    "\n",
    "def extract_jj(tweet):\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    tag = ct.tag_sents([words])\n",
    "    flat_tag = [item for sublist in tag for item in sublist]\n",
    "    pos_count = Counter([j for i,j in flat_tag])\n",
    "    return pos_count['JJ']\n",
    "\n",
    "def extract_neg(tweet):\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    tag = ct.tag_sents([words])\n",
    "    flat_tag = [item for sublist in tag for item in sublist]\n",
    "    pos_count = Counter([j for i,j in flat_tag])\n",
    "    return pos_count['NEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = set()\n",
    "\n",
    "def extract_negative_lexicon(tweet):\n",
    "    score = 0\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    tag = ct.tag_sents([words])\n",
    "    flat_tag = [item for sublist in tag for item in sublist]\n",
    "    words_tag = dict()\n",
    "    for w, tg in flat_tag:\n",
    "        if tg == 'JJ':\n",
    "            jj.add(w)\n",
    "            words_tag[w] = tg\n",
    "    for i in range(len(words)):\n",
    "        if i > 1 and words[i-2] == 'tidak' and words[i] in positives:\n",
    "            score += 1\n",
    "    for negative in negatives:\n",
    "        for i in range(len(words)):\n",
    "            if words[i] == negative:\n",
    "                if (i > 0):\n",
    "                    if (words[i-1] != 'tidak'):\n",
    "                        score += 1\n",
    "#                         if words[i] in words_tag.keys() and words_tag[words[i]] == 'JJ':\n",
    "#                             score += 0.5\n",
    "                        \n",
    "                else:\n",
    "                    score += 1\n",
    "    for positive in positives:\n",
    "        if ('tidak ' + positive) in tweet:\n",
    "            score += 1\n",
    "        if ('jangan ' + positive) in tweet:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_positive_lexicon(tweet):\n",
    "    score = 0\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    tag = ct.tag_sents([words])\n",
    "    flat_tag = [item for sublist in tag for item in sublist]\n",
    "    words_tag = dict()\n",
    "    for w, tg in flat_tag:\n",
    "        if tg == 'JJ':\n",
    "            jj.add(w)\n",
    "            words_tag[w] = tg\n",
    "    for i in range(len(words)):\n",
    "        if i > 1 and words[i-2] == 'tidak' and words[i] in negatives:\n",
    "            score += 1\n",
    "    for positive in positives:\n",
    "        for i in range(len(words)):\n",
    "            if words[i] == positive:\n",
    "                if (i > 0):\n",
    "                    if (words[i-1] != 'tidak'):\n",
    "                        score += 1\n",
    "#                         if words[i] in words_tag.keys() and words_tag[words[i]] == 'JJ':\n",
    "#                             score += 0.5\n",
    "                else:\n",
    "                    score += 1\n",
    "            \n",
    "    for negative in negatives:\n",
    "        if ('tidak ' + negative) in tweet:\n",
    "            score += 1\n",
    "        if ('jangan ' + negative) in tweet:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_positive_lexicon('tidak kreatif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(data):\n",
    "    temp_data = data.copy()\n",
    "    temp_data['lexicon_pos_score'] = temp_data['tweet'].apply(extract_positive_lexicon)\n",
    "    temp_data['lexicon_neg_score'] = temp_data['tweet'].apply(extract_negative_lexicon)\n",
    "    temp_dat_2 = data.copy()\n",
    "    temp_dat_2['lexicon_score'] = (temp_data['lexicon_pos_score'] - temp_data['lexicon_neg_score'])\n",
    "    temp_dat_2['jj'] = temp_dat_2['tweet'].apply(extract_jj)\n",
    "    temp_dat_2['neg'] = temp_dat_2['tweet'].apply(extract_neg)\n",
    "    return temp_dat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_extracted = extract_feature(train_data_preprocess)\n",
    "tester_data_extracted = extract_feature(tester_data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_cond = (train_data_extracted['sentimen'] == 0) & (train_data_extracted['lexicon_score'] == 0) & (train_data_extracted['emoticon_score'] == 0)\n",
    "# train_data_extracted['lexicon_score'].iloc[zero_cond.values] = -2 \n",
    "\n",
    "# zero_cond = (tester_data_extracted['lexicon_score'] == 0) & (tester_data_extracted['emoticon_score'] == 0)\n",
    "# tester_data_extracted['lexicon_score'].iloc[zero_cond.values] = -2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['lexicon_score', 'emoticon_score', 'jj', 'neg']\n",
    "target = 'sentimen'\n",
    "\n",
    "X, y = train_data_extracted[features].values, train_data_extracted[target].values\n",
    "Xx = tester_data_extracted[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "classifiers = [\n",
    "    ('Decission Tree', DecisionTreeClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('SVM', LinearSVC()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('Ensemble', GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_scores = 0\n",
    "test_scores = 0\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=46)\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_scores += accuracy_score(dt.predict(X_train), y_train)\n",
    "    test_scores += accuracy_score(dt.predict(X_test), y_test)\n",
    "    \n",
    "print(train_scores / 10)\n",
    "print(test_scores / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = dt.predict(X)\n",
    "train_data_extracted['predicted'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_extracted[train_data_extracted['predicted'] != train_data_extracted['sentimen']].head(40).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_predicted = dt.predict(Xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_data_extracted['predicted'] = tester_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_data_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_data_extracted.to_csv('results8.csv', header=False, index=False, columns=['test_ID', 'predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_emoticon(\"cie andien yang lagi bep marah marah mulu:p :) :) :) :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_positive_lexicon('tidak kreatif ambil kutipan orang tertawa suka iya ungkapin ditikung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_positive_lexicon('tidak kreatif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
